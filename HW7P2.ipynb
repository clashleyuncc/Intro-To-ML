{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPvkbu3t9qbLbeAe9az9QiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clashleyuncc/Intro-To-ML/blob/main/HW7P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "data_path = '../data-unversioned/p1ch7/'\n",
        "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
        "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "  def __init__(self, n_chans):#16\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
        "                          padding=1, bias=False)\n",
        "    self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "    torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                  nonlinearity='relu')\n",
        "    torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "    torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.batch_norm(out)\n",
        "    out = torch.relu(out)\n",
        "    return out + x\n",
        "\n",
        "class ResNet10(nn.Module):\n",
        "  def __init__(self, n_chans1=32, n_blocks=10):\n",
        "    super().__init__()\n",
        "    self.n_chans1 = n_chans1\n",
        "    self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "    self.resblocks = nn.Sequential(\n",
        "        *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "    self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "    self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "    out = self.resblocks(out)\n",
        "    out = F.max_pool2d(out, 2)\n",
        "    out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "    out = torch.relu(self.fc1(out))\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    loss_train = 0.0\n",
        "    for imgs, labels in train_loader:\n",
        "      imgs = imgs.to(device=device)\n",
        "      labels = labels.to(device=device)\n",
        "      outputs = model(imgs)\n",
        "      loss = loss_fn(outputs, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_train += loss.item()\n",
        "    if epoch == 1:\n",
        "      print('Esimated run time for {} Epoch: {} minutes'.format(\n",
        "            n_epochs, round((time.time()-start_time)*n_epochs/60,3)))\n",
        "    if epoch == 1 or epoch % 10 == 0:\n",
        "      print('{} Seconds, Epoch {}, Training Loss {}'.format(\n",
        "          time.time()-start_time, epoch,\n",
        "          loss_train / len(train_loader)))\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "tensor_cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
        "                                  transform=transforms.ToTensor())\n",
        "\n",
        "transformed_cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=False,\n",
        "    transform = transforms.Compose([\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                            (0.2470, 0.2435, 0.2616))\n",
        "    ]))\n",
        "\n",
        "tensor_cifar10 = transformed_cifar10\n",
        "\n",
        "model = ResNet10().to(device=device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(tensor_cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "  for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "    print(\"Accuracy {}: {:.2f}\".format(name , correct/total))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(tensor_cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(tensor_cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "validate(model, train_loader, val_loader)\n",
        "\n",
        "print(f\"Training Time: {time.time()-start_time} seconds\")\n",
        "print(f\"Approximatly: {round((time.time()-start_time)/60,3)} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8984YUouytOQ",
        "outputId": "b6788f24-c8fc-4ff2-af1d-867ebb236ae1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Esimated run time for 200 Epoch: 65.257 minutes\n",
            "19.57721781730652 Seconds, Epoch 1, Training Loss 1.7372073159193444\n",
            "161.4488546848297 Seconds, Epoch 10, Training Loss 0.919807996324566\n",
            "313.96229243278503 Seconds, Epoch 20, Training Loss 0.7063378002637487\n",
            "472.88633370399475 Seconds, Epoch 30, Training Loss 0.5731973091278539\n",
            "632.3554894924164 Seconds, Epoch 40, Training Loss 0.4676260417303466\n",
            "790.1446046829224 Seconds, Epoch 50, Training Loss 0.375128383755379\n",
            "948.7587575912476 Seconds, Epoch 60, Training Loss 0.301872299569647\n",
            "1104.570143699646 Seconds, Epoch 70, Training Loss 0.23517241163174515\n",
            "1258.865888595581 Seconds, Epoch 80, Training Loss 0.18812784272939195\n",
            "1411.115600824356 Seconds, Epoch 90, Training Loss 0.1594656245673404\n",
            "1559.4171395301819 Seconds, Epoch 100, Training Loss 0.13217191253205204\n",
            "1711.2565841674805 Seconds, Epoch 110, Training Loss 0.09023544567582362\n",
            "1871.2344119548798 Seconds, Epoch 120, Training Loss 0.09513430459582059\n",
            "2022.0385975837708 Seconds, Epoch 130, Training Loss 0.05175426967746919\n",
            "2176.1598930358887 Seconds, Epoch 140, Training Loss 0.1639212462640203\n",
            "2336.4312653541565 Seconds, Epoch 150, Training Loss 0.02110348360272377\n",
            "2493.9538538455963 Seconds, Epoch 160, Training Loss 0.011028906700553854\n",
            "2645.464508533478 Seconds, Epoch 170, Training Loss 0.06442487019452425\n",
            "2794.661631345749 Seconds, Epoch 180, Training Loss 0.009644389914660155\n",
            "2952.0590550899506 Seconds, Epoch 190, Training Loss 0.005313363608733281\n",
            "3112.6273148059845 Seconds, Epoch 200, Training Loss 0.012366926262784591\n",
            "Accuracy train: 0.71\n",
            "Accuracy val: 0.35\n",
            "Training Time: 3128.260771751404 seconds\n",
            "Approximatly: 52.138 minutes\n"
          ]
        }
      ]
    }
  ]
}